{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2rGnGinFGWB"
      },
      "source": [
        "# Scribing Assignment - Part II"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTAbwOuLeVD6"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1_LdN0VeVd0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1Dle1E3BmP0",
        "outputId": "58966109-07fd-428b-eb4d-777c10552b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "## mount to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyh-OsKteaRK"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJIpX3JBecI1"
      },
      "outputs": [],
      "source": [
        "# load text and covert to lowercase\n",
        "filename = '/content/drive/MyDrive/Dataset/assignment4/testdata.txt'\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()  ## convert the text to lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XoNMGi0eqCE",
        "outputId": "7109fe1c-7a15-4b69-eb94-86e995d4391c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "three rings for the elven-kings under the sky,\n",
            "               seven for the dwarf-lords in their halls of stone,\n",
            "            nine for mortal men doomed to die,\n",
            "              one for the dark lord on his dark throne\n",
            "           in the land of mordor where the shadows lie.\n",
            "               one ring to rule them all, one ring to find them,\n",
            "               one ring to bring them all and in the darkness bind them\n",
            "           in the land of mordor where the shadows lie.\n",
            "           \n",
            "foreword\n",
            "\n",
            "this tale grew in the telling, until it became a history of the great war of the ring and included many glimpses of the yet more ancient history that preceded it. it was begun soon after _the hobbit_ was written and before its publication in 1937; but i did not go on with this sequel, for i wished first to complete and set in order the mythology and legends of the elder days, which had then been taking shape for some years. i desired to do this for my own satisfaction, and i had little hope that other people \n"
          ]
        }
      ],
      "source": [
        "print(raw_text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsGLm97jTviy"
      },
      "source": [
        "## data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy_BddtegAAM"
      },
      "outputs": [],
      "source": [
        "raw_text = raw_text.replace('\\n\\n','\\n').replace('\\n','|')  ## | is used to represent EOS --> END OF SENTENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "W1P-8ceMgMtm",
        "outputId": "3620f0a5-f785-4578-98ba-efddc4fd733b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'three rings for the elven-kings under the sky,|               seven for the dwarf-lords in their halls of stone,|            nine for mortal men doomed to die,|              one for the dark lord on his dark throne|           in the land of mordor where the shadows lie.|               one ring to rule them all, one ring to find them,|               one ring to bring them all and in the darkness bind them|           in the land of mordor where the shadows lie.|           |foreword|this tale grew in the telling, until it became a history of the great war of the ring and included many glimpses of the yet more ancient history that preceded it. it was begun soon after _the hobbit_ was written and before its publication in 1937; but i did not go on with this sequel, for i wished first to complete and set in order the mythology and legends of the elder days, which had then been taking shape for some years. i desired to do this for my own satisfaction, and i had little hope that other people w'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_text[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwFs5gN2Ujom",
        "outputId": "f8deb345-e2c0-48e7-fec1-117a9e23a363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ],
      "source": [
        "## PUNCTUATION REMOVAL\n",
        "import string\n",
        "\n",
        "punctuations = string.punctuation\n",
        "print(punctuations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB8CK2GyZUJn",
        "outputId": "cc08df93-9e78-4559-a52f-90d541c801d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{}~\n"
          ]
        }
      ],
      "source": [
        "punctuations = punctuations.replace('|','')  ## replace '|' from string of punctuations. we need '|' to represent EOS\n",
        "print(punctuations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSNqH69mV77O"
      },
      "outputs": [],
      "source": [
        "def remove_punc(text):\n",
        "    # Create a translation table to remove punctuation characters\n",
        "    translator = str.maketrans('','', punctuations)\n",
        "\n",
        "    # Use the translation table to remove punctuation\n",
        "    text = text.translate(translator)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGRQrRHUVTIB"
      },
      "outputs": [],
      "source": [
        "raw_text = remove_punc(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "XYanBb4Fa70K",
        "outputId": "8b264e0f-3add-4c3b-e824-18d9610797dc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'three rings for the elvenkings under the sky|               seven for the dwarflords in their halls of stone|            nine for mortal men doomed to die|              one for the dark lord on his dark throne|           in the land of mordor where the shadows lie|               one ring to rule them all one ring to find them|               one ring to bring them all and in the darkness bind them|           in the land of mordor where the shadows lie|           |foreword|this tale grew in the telling until it became a history of the great war of the ring and included many glimpses of the yet more ancient history that preceded it it was begun soon after the hobbit was written and before its publication in 1937 but i did not go on with this sequel for i wished first to complete and set in order the mythology and legends of the elder days which had then been taking shape for some years i desired to do this for my own satisfaction and i had little hope that other people would be interested'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_text[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmUUr8ube-wl"
      },
      "source": [
        "### create mapping of unique chars to integers and integers to characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmLpzW6KfBVn"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i,c) for i,c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Kwcn5RONShg",
        "outputId": "34a6a8fb-fefe-4194-db86-576f923c1a80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{' ': 0,\n",
              " '0': 1,\n",
              " '1': 2,\n",
              " '2': 3,\n",
              " '3': 4,\n",
              " '4': 5,\n",
              " '5': 6,\n",
              " '6': 7,\n",
              " '7': 8,\n",
              " '8': 9,\n",
              " '9': 10,\n",
              " 'a': 11,\n",
              " 'b': 12,\n",
              " 'c': 13,\n",
              " 'd': 14,\n",
              " 'e': 15,\n",
              " 'f': 16,\n",
              " 'g': 17,\n",
              " 'h': 18,\n",
              " 'i': 19,\n",
              " 'j': 20,\n",
              " 'k': 21,\n",
              " 'l': 22,\n",
              " 'm': 23,\n",
              " 'n': 24,\n",
              " 'o': 25,\n",
              " 'p': 26,\n",
              " 'q': 27,\n",
              " 'r': 28,\n",
              " 's': 29,\n",
              " 't': 30,\n",
              " 'u': 31,\n",
              " 'v': 32,\n",
              " 'w': 33,\n",
              " 'x': 34,\n",
              " 'y': 35,\n",
              " 'z': 36,\n",
              " '|': 37,\n",
              " '\\x96': 38,\n",
              " 'á': 39,\n",
              " 'â': 40,\n",
              " 'ä': 41,\n",
              " 'é': 42,\n",
              " 'ë': 43,\n",
              " 'í': 44,\n",
              " 'ó': 45,\n",
              " 'ú': 46,\n",
              " 'û': 47}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "char_to_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6TEwHHnM5i1",
        "outputId": "d3de5309-0759-4d2c-eb2f-0ddadab71d0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: ' ',\n",
              " 1: '0',\n",
              " 2: '1',\n",
              " 3: '2',\n",
              " 4: '3',\n",
              " 5: '4',\n",
              " 6: '5',\n",
              " 7: '6',\n",
              " 8: '7',\n",
              " 9: '8',\n",
              " 10: '9',\n",
              " 11: 'a',\n",
              " 12: 'b',\n",
              " 13: 'c',\n",
              " 14: 'd',\n",
              " 15: 'e',\n",
              " 16: 'f',\n",
              " 17: 'g',\n",
              " 18: 'h',\n",
              " 19: 'i',\n",
              " 20: 'j',\n",
              " 21: 'k',\n",
              " 22: 'l',\n",
              " 23: 'm',\n",
              " 24: 'n',\n",
              " 25: 'o',\n",
              " 26: 'p',\n",
              " 27: 'q',\n",
              " 28: 'r',\n",
              " 29: 's',\n",
              " 30: 't',\n",
              " 31: 'u',\n",
              " 32: 'v',\n",
              " 33: 'w',\n",
              " 34: 'x',\n",
              " 35: 'y',\n",
              " 36: 'z',\n",
              " 37: '|',\n",
              " 38: '\\x96',\n",
              " 39: 'á',\n",
              " 40: 'â',\n",
              " 41: 'ä',\n",
              " 42: 'é',\n",
              " 43: 'ë',\n",
              " 44: 'í',\n",
              " 45: 'ó',\n",
              " 46: 'ú',\n",
              " 47: 'û'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "int_to_char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZmtLO5UcJEI",
        "outputId": "4260db8c-33ca-446b-f03a-e6c93f03c1b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[' ',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '|',\n",
              " '\\x96',\n",
              " 'á',\n",
              " 'â',\n",
              " 'ä',\n",
              " 'é',\n",
              " 'ë',\n",
              " 'í',\n",
              " 'ó',\n",
              " 'ú',\n",
              " 'û']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3PLYr8bgF81"
      },
      "source": [
        "### summarize the loaded data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9mnbwv6gEmt",
        "outputId": "1589b1ad-4115-46fa-d90e-72c19764360f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Characters:  982155\n",
            "Total Vocab:  48\n"
          ]
        }
      ],
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxaLQ3RvgVGE"
      },
      "source": [
        "# prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGeboG0SgZdb",
        "outputId": "a8a86477-b14d-44a5-866c-df928237cec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Patterns:  982148\n"
          ]
        }
      ],
      "source": [
        "seq_length = 7  ## length of one sequence /timesteps\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i:i + seq_length]\n",
        "    seq_out = raw_text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)  ## number of patters / datapoints\n",
        "print(\"Total Patterns: \", n_patterns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFzbkyCsgupw",
        "outputId": "cd1d4a18-56d0-4b59-9dc6-8d420f099358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[30, 18, 28, 15, 15, 0, 28]\n",
            "19\n"
          ]
        }
      ],
      "source": [
        "## print a sample datapoint. You can uncomment to see the sample\n",
        "print(dataX[0])\n",
        "print(dataY[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20hH04FChHo5"
      },
      "outputs": [],
      "source": [
        "# reshape X to be [samples, time steps] --> embedding layer in LSTM accepts the inputs in this shape only.\n",
        "# timesteps = sequence length\n",
        "# convert dataX to torch tensor and reshaping\n",
        "X = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\n",
        "# X = X / float(n_vocab)\n",
        "# convert dataY to torch tensor\n",
        "y = torch.tensor(dataY, dtype=torch.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzy-7COhP3Fx",
        "outputId": "aef1d403-74fe-43ca-a731-13805eac4507"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[30.],\n",
              "         [18.],\n",
              "         [28.],\n",
              "         ...,\n",
              "         [15.],\n",
              "         [ 0.],\n",
              "         [28.]],\n",
              "\n",
              "        [[18.],\n",
              "         [28.],\n",
              "         [15.],\n",
              "         ...,\n",
              "         [ 0.],\n",
              "         [28.],\n",
              "         [19.]],\n",
              "\n",
              "        [[28.],\n",
              "         [15.],\n",
              "         [15.],\n",
              "         ...,\n",
              "         [28.],\n",
              "         [19.],\n",
              "         [24.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[30.],\n",
              "         [18.],\n",
              "         [15.],\n",
              "         ...,\n",
              "         [21.],\n",
              "         [19.],\n",
              "         [24.]],\n",
              "\n",
              "        [[18.],\n",
              "         [15.],\n",
              "         [ 0.],\n",
              "         ...,\n",
              "         [19.],\n",
              "         [24.],\n",
              "         [17.]],\n",
              "\n",
              "        [[15.],\n",
              "         [ 0.],\n",
              "         [21.],\n",
              "         ...,\n",
              "         [24.],\n",
              "         [17.],\n",
              "         [37.]]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v107-phd6HH",
        "outputId": "e8668a9e-25d7-4363-bc0b-816dc0d60a55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([982148, 7, 1])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psetrHaTQFOq",
        "outputId": "dd5b20ed-b1e4-4123-a397-ebd628aec67d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([19, 24, 17,  ..., 17, 37, 37])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9ZKVqNQQKdy",
        "outputId": "740c5f80-a8e5-49b1-be3d-5e9c1f603e21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([982148])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHK53pLdFdj9"
      },
      "source": [
        "## Write code to prepare a train, val and test split from X and y to create\n",
        "\n",
        "X_train, y_train\n",
        "\n",
        "X_val, y_val\n",
        "\n",
        "X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdr939gdQhSi",
        "outputId": "d77ee303-f794-4e04-d316-ffa508219d58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 589288\n",
            "Validation set size: 196430\n",
            "Test set size: 196430\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into test (80%) and temporary (20%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split the temporary data into training (80%) and validation (20%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
        "\n",
        "# Print the sizes of the splits\n",
        "print(\"Training set size:\", len(X_train))\n",
        "print(\"Validation set size:\", len(X_val))\n",
        "print(\"Test set size:\", len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B53f_ErOh2HJ"
      },
      "source": [
        "# Create NN architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzl7yfYkGS7A"
      },
      "source": [
        "## Custom LSTM network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BurEkeMph6wK"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "parameters of LSTM/RNN/GRU layer:\n",
        "\n",
        "input_size – The number of expected features in the input x\n",
        "\n",
        "hidden_size – The number of features in the hidden state h\n",
        "\n",
        "num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM,\n",
        "with the second LSTM taking in outputs of the first LSTM and computing the final results\n",
        "\n",
        "batch_first – If True, then the input and output tensors are provided as (batch, seq, feature)\n",
        "\n",
        "\"\"\"\n",
        "######\n",
        "# hyperparameters\n",
        "#lstm_embeding_dim = ??\n",
        "#lstm_hid_size = ??\n",
        "#lstm_layers=  ??\n",
        "\n",
        "\n",
        "######\n",
        "\n",
        "class CharModel_LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #Create a suitable lstm network with an embedding layer, 2 or 3 lstm layers\n",
        "        #a linear layer. You can have a dropout if required.\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear = nn.Linear(256, n_vocab)\n",
        "    def forward(self, x):\n",
        "        #Write the forward pass for your network\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, -1,:]\n",
        "        x = self.linear(self.dropout(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZmhhmyOGXGT"
      },
      "source": [
        "## Custom RNN Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUCKQgkqGJpa"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "parameters of LSTM/RNN/GRU layer:\n",
        "\n",
        "input_size – The number of expected features in the input x\n",
        "\n",
        "hidden_size – The number of features in the hidden state h\n",
        "\n",
        "num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN,\n",
        "with the second RNN taking in outputs of the first RNN and computing the final results\n",
        "\n",
        "batch_first – If True, then the input and output tensors are provided as (batch, seq, feature)\n",
        "\n",
        "\"\"\"\n",
        "######\n",
        "# hyperparameters\n",
        "#rnn_embeding_dim = ??\n",
        "#rnn_hid_size = ??\n",
        "#rnn_layers=  ??\n",
        "\n",
        "\n",
        "######\n",
        "\n",
        "class CharModel_RNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #Create a suitable rnn network with an embedding layer, 2 or 3 rnn layers\n",
        "        #a linear layer. You can have a dropout if required.\n",
        "    def forward(self, x):\n",
        "        #Write the forward pass for your network\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul_0zFEfGb3q"
      },
      "source": [
        "## Custom GRU Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeBoWC3TGaqP"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "parameters of LSTM/RNN/GRU layer:\n",
        "\n",
        "input_size – The number of expected features in the input x\n",
        "\n",
        "hidden_size – The number of features in the hidden state h\n",
        "\n",
        "num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRU together to form a stacked GRU,\n",
        "with the second GRU taking in outputs of the first GRU and computing the final results\n",
        "\n",
        "batch_first – If True, then the input and output tensors are provided as (batch, seq, feature)\n",
        "\n",
        "\"\"\"\n",
        "######\n",
        "# hyperparameters\n",
        "#gru_embeding_dim = ??\n",
        "#gru_hid_size = ??\n",
        "#gru_layers=  ??\n",
        "\n",
        "\n",
        "######\n",
        "\n",
        "class CharModel_GRU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #Create a suitable gru network with an embedding layer, 2 or 3 gru layers\n",
        "        #a linear layer. You can have a dropout if required.\n",
        "    def forward(self, x):\n",
        "        #Write the forward pass for your network\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7hNtilqictV"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeZ91GodGygG"
      },
      "source": [
        "1. Write code to train the LSTM network and store the model as a checkpoint. Use validation data to tune your hyperparameters learning rate, batch size, num epochs, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASHx0j8EigEW",
        "outputId": "d636a95b-7f16-48bc-dbf9-a7490608a57e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda available\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CharModel_LSTM(\n",
              "  (lstm): LSTM(1, 256, batch_first=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (linear): Linear(in_features=256, out_features=48, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = CharModel_LSTM()\n",
        "if torch.cuda.is_available:\n",
        "  print('cuda available')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18dojtvli7NT"
      },
      "outputs": [],
      "source": [
        "# Set up the parameters for training,\n",
        "# tune the tunable parameters if necessary using val data\n",
        "batch_size = 40\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "train_loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
        "val_loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
        "test_loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMyMZYkjjWgu",
        "outputId": "7d930649-69eb-4df3-ad12-7117e0d83518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Cross-entropy: 1169528.6250\n",
            "Epoch 1: Cross-entropy: 1069955.0000\n",
            "Epoch 2: Cross-entropy: 1014340.9375\n",
            "Epoch 3: Cross-entropy: 975519.1875\n",
            "Epoch 4: Cross-entropy: 948310.5625\n",
            "Epoch 5: Cross-entropy: 929056.5625\n",
            "Epoch 6: Cross-entropy: 907094.1875\n",
            "Epoch 7: Cross-entropy: 898178.5000\n",
            "Epoch 8: Cross-entropy: 886649.0625\n",
            "Epoch 9: Cross-entropy: 875138.4375\n",
            "Epoch 10: Cross-entropy: 865574.8750\n",
            "Epoch 11: Cross-entropy: 859753.1875\n",
            "Epoch 12: Cross-entropy: 852201.0625\n",
            "Epoch 13: Cross-entropy: 847829.8125\n",
            "Epoch 14: Cross-entropy: 846016.8125\n",
            "Epoch 15: Cross-entropy: 837068.1875\n",
            "Epoch 16: Cross-entropy: 833059.5625\n",
            "Epoch 17: Cross-entropy: 828265.6250\n",
            "Epoch 18: Cross-entropy: 823955.5000\n",
            "Epoch 19: Cross-entropy: 819510.4375\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "n_epochs = 4\n",
        "best_model = None\n",
        "### ckpt_path =  # set up check point path\n",
        "## If you have stored a checkpoint, In this way you can resume training model after the last training step by just loading it from the directory given.\n",
        "# if os.path.isfile(ckpt_path):\n",
        "#     model.load_state_dict(torch.load(ckpt_path)[0])\n",
        "#     print('loading from model check point')\n",
        "\n",
        "best_loss = np.inf\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    #Write training code\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        y_pred = model(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred.to(device), y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Validation\n",
        "    # Writevalidation code and tune parameters if necessary\n",
        "    #Include codes to print necessary logs\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            y_pred = model(X_batch.to(device))\n",
        "            loss += loss_fn(y_pred.to(device), y_batch.to(device))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model = model.state_dict()\n",
        "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n",
        "torch.save([best_model, char_to_int], \"single-char.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Arfwa2N2keC5"
      },
      "source": [
        "# Generating character using trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X3qDS7mkjIb"
      },
      "outputs": [],
      "source": [
        "best_model, char_to_int = torch.load(\"single-char.pth\")  ## loading saved model\n",
        "n_vocab = len(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
        "model.load_state_dict(best_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D0OzgmvNJKk"
      },
      "outputs": [],
      "source": [
        "X_test.size()\n",
        "np.random.choice(len(X_test))\n",
        "char_to_int['g']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp0LLY0neFBw"
      },
      "outputs": [],
      "source": [
        "#Test your model's generative performance using the following testing code\n",
        "# randomly generate a prompt\n",
        "n = 10 ## number of characters to predict\n",
        "rand_index = np.random.choice(len(X_test))\n",
        "prompt = X_test[rand_index]\n",
        "next_char_actual = y_test[rand_index]\n",
        "\n",
        "#pattern = [char_to_int[c] for c in prompt]\n",
        "\n",
        "#---------------------------------------------------------------------------#\n",
        "#prediction\n",
        "\n",
        "model.eval()\n",
        "print('Prompt: \"%s\"' % prompt)\n",
        "#print('Next chars (actual):',next_char_actual )\n",
        "#print('----')\n",
        "#print('prediction:\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHvZHVAeVrUc"
      },
      "outputs": [],
      "source": [
        "#Use your model to predict the next seq_length characters and check the performance\n",
        "# CODE HERE\n",
        "seq_length = 7\n",
        "raw_text = open('/content/drive/MyDrive/ie643ss/Question2_dataset.txt', 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "raw_text = remove_punc(raw_text)\n",
        "start = np.random.randint(0, len(raw_text)-seq_length)\n",
        "prompt = raw_text[start:start+seq_length]\n",
        "pattern = [char_to_int[c] for c in prompt]\n",
        "with torch.no_grad():\n",
        "    for i in range(n):\n",
        "        # format input array of int into PyTorch tensor\n",
        "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "        # generate logits as output from the model\n",
        "        prediction = model(x.to(device))\n",
        "        # convert logits into one character\n",
        "        index = int(prediction.argmax())\n",
        "        result = int_to_char[index]\n",
        "        print(result, end=\"\")\n",
        "        # append the new character into the prompt for the next iteration\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:]\n",
        "print()\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFPz5NcTIhs7"
      },
      "outputs": [],
      "source": [
        "#Write code to evaluate your model's performance on the full test data\n",
        "#Use character error rate on the test data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR1H4gp7I1Ax"
      },
      "source": [
        "2. Using the template above, train the RNN network and store the model as a checkpoint. Use validation data to tune your hyperparameters learning rate, batch size, num epochs, etc. And test the models performance on an arbitrary test data point (or a set of test data points). Also evaluate the performance on full test data using character error rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNBINVP1JCIW"
      },
      "source": [
        "3. Using the template above, train the GRU network and store the model as a checkpoint. Use validation data to tune your hyperparameters learning rate, batch size, num epochs, etc. And test the models performance on an arbitrary test data point (or a set of test data points). Also evaluate the performance on full test data using character error rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twqdwH0mJAOg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}