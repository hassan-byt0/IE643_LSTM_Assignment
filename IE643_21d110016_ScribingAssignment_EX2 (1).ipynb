{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scribing Assignment - Part II"
      ],
      "metadata": {
        "id": "h2rGnGinFGWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "CTAbwOuLeVD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data"
      ],
      "metadata": {
        "id": "y1_LdN0VeVd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## mount to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "r1Dle1E3BmP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563ef015-ef74-42d8-d22f-28a069b88b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "Cyh-OsKteaRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Dataset/assignment4/testdata.txt'"
      ],
      "metadata": {
        "id": "QJiRkkUEFDrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text file\n",
        "with open(path, 'r',encoding='utf-8') as file:\n",
        "    raw_text = file.read()\n",
        "\n",
        "# Print the content of the text file\n",
        "raw_text = raw_text.lower()\n",
        "\n"
      ],
      "metadata": {
        "id": "toR7BeD5GSGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load text and covert to lowercase\n",
        "#filename = \"textdata.txt\"\n",
        "#raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "#raw_text = raw_text.lower()  ## convert the text to lower case"
      ],
      "metadata": {
        "id": "UJIpX3JBecI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_text[:1000])"
      ],
      "metadata": {
        "id": "2XoNMGi0eqCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66635744-95d5-45bf-d8af-d38e235325b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "three rings for the elven-kings under the sky,\n",
            "               seven for the dwarf-lords in their halls of stone,\n",
            "            nine for mortal men doomed to die,\n",
            "              one for the dark lord on his dark throne\n",
            "           in the land of mordor where the shadows lie.\n",
            "               one ring to rule them all, one ring to find them,\n",
            "               one ring to bring them all and in the darkness bind them\n",
            "           in the land of mordor where the shadows lie.\n",
            "           \n",
            "foreword\n",
            "\n",
            "this tale grew in the telling, until it became a history of the great war of the ring and included many glimpses of the yet more ancient history that preceded it. it was begun soon after _the hobbit_ was written and before its publication in 1937; but i did not go on with this sequel, for i wished first to complete and set in order the mythology and legends of the elder days, which had then been taking shape for some years. i desired to do this for my own satisfaction, and i had little hope that other people \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data preprocessing"
      ],
      "metadata": {
        "id": "UsGLm97jTviy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = raw_text.replace('\\n\\n','\\n').replace('\\n','|')  ## | is used to represent EOS --> END OF SENTENCE"
      ],
      "metadata": {
        "id": "Iy_BddtegAAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text[:1000]"
      ],
      "metadata": {
        "id": "W1P-8ceMgMtm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "65e9770e-bf44-4be0-da35-da92797a8883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'three rings for the elven-kings under the sky,|               seven for the dwarf-lords in their halls of stone,|            nine for mortal men doomed to die,|              one for the dark lord on his dark throne|           in the land of mordor where the shadows lie.|               one ring to rule them all, one ring to find them,|               one ring to bring them all and in the darkness bind them|           in the land of mordor where the shadows lie.|           |foreword|this tale grew in the telling, until it became a history of the great war of the ring and included many glimpses of the yet more ancient history that preceded it. it was begun soon after _the hobbit_ was written and before its publication in 1937; but i did not go on with this sequel, for i wished first to complete and set in order the mythology and legends of the elder days, which had then been taking shape for some years. i desired to do this for my own satisfaction, and i had little hope that other people w'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## PUNCTUATION REMOVAL\n",
        "import string\n",
        "\n",
        "punctuations = string.punctuation\n",
        "print(punctuations)"
      ],
      "metadata": {
        "id": "kwFs5gN2Ujom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6ecfb5-44d7-40d6-84dc-a3b6458e1d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punctuations = punctuations.replace('|','')  ## replace '|' from string of punctuations. we need '|' to represent EOS\n",
        "print(punctuations)"
      ],
      "metadata": {
        "id": "qB8CK2GyZUJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c03405-1483-4b0d-8090-8baf356e91ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(text):\n",
        "    # Create a translation table to remove punctuation characters\n",
        "    translator = str.maketrans('','', punctuations)\n",
        "\n",
        "    # Use the translation table to remove punctuation\n",
        "    text = text.translate(translator)\n",
        "    return text"
      ],
      "metadata": {
        "id": "cSNqH69mV77O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = remove_punc(raw_text)"
      ],
      "metadata": {
        "id": "lGRQrRHUVTIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text[:1000]"
      ],
      "metadata": {
        "id": "XYanBb4Fa70K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "f82e36d5-b156-48fb-e662-7bf23fd7fb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'three rings for the elvenkings under the sky|               seven for the dwarflords in their halls of stone|            nine for mortal men doomed to die|              one for the dark lord on his dark throne|           in the land of mordor where the shadows lie|               one ring to rule them all one ring to find them|               one ring to bring them all and in the darkness bind them|           in the land of mordor where the shadows lie|           |foreword|this tale grew in the telling until it became a history of the great war of the ring and included many glimpses of the yet more ancient history that preceded it it was begun soon after the hobbit was written and before its publication in 1937 but i did not go on with this sequel for i wished first to complete and set in order the mythology and legends of the elder days which had then been taking shape for some years i desired to do this for my own satisfaction and i had little hope that other people would be interested'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create mapping of unique chars to integers and integers to characters"
      ],
      "metadata": {
        "id": "lmUUr8ube-wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i,c) for i,c in enumerate(chars))"
      ],
      "metadata": {
        "id": "dmLpzW6KfBVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars"
      ],
      "metadata": {
        "id": "AZmtLO5UcJEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a563da00-b752-423d-81bc-d054cf61254d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '|',\n",
              " '\\x96',\n",
              " 'á',\n",
              " 'â',\n",
              " 'ä',\n",
              " 'é',\n",
              " 'ë',\n",
              " 'í',\n",
              " 'ó',\n",
              " 'ú',\n",
              " 'û']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### summarize the loaded data"
      ],
      "metadata": {
        "id": "B3PLYr8bgF81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "metadata": {
        "id": "G9mnbwv6gEmt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3db0c7f6-44e4-4e13-be68-da7103c6b983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  982155\n",
            "Total Vocab:  48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prepare the dataset"
      ],
      "metadata": {
        "id": "sxaLQ3RvgVGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 7  ## length of one sequence /timesteps\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i:i + seq_length]\n",
        "    seq_out = raw_text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)  ## number of patters / datapoints\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "metadata": {
        "id": "MGeboG0SgZdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd775736-bcd1-47d5-b6f8-4439770316a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns:  982148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## print a sample datapoint. You can uncomment to see the sample\n",
        "dataX[0]\n",
        "dataY[0]"
      ],
      "metadata": {
        "id": "wFzbkyCsgupw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9e52684-3a4b-4f5c-9661-d9bc17808af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape X to be [samples, time steps] --> embedding layer in LSTM accepts the inputs in this shape only.\n",
        "#timesteps = sequence length\n",
        "# convert dataX to torch tensor and reshaping\n",
        "X = torch.tensor(dataX, dtype=torch.int32).reshape(n_patterns, seq_length)\n",
        "\n",
        "# convert dataY to torch tensor\n",
        "y = torch.tensor(dataY)"
      ],
      "metadata": {
        "id": "20hH04FChHo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "_v107-phd6HH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ac4ebb-1d47-4856-c2b0-627da9b796de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([982148, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write code to prepare a train, val and test split from X and y to create\n",
        "\n",
        "X_train, y_train\n",
        "\n",
        "X_val, y_val\n",
        "\n",
        "X_test, y_test"
      ],
      "metadata": {
        "id": "eHK53pLdFdj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into test (80%) and temporary (20%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split the temporary data into training (80%) and validation (20%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
        "\n",
        "# Print the sizes of the splits\n",
        "print(\"Training set size:\", len(X_train))\n",
        "print(\"Validation set size:\", len(X_val))\n",
        "print(\"Test set size:\", len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CtMcdHBThH-",
        "outputId": "f8bc21e8-a293-43e8-c00e-efc148fc5427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 589288\n",
            "Validation set size: 196430\n",
            "Test set size: 196430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create NN architecture"
      ],
      "metadata": {
        "id": "B53f_ErOh2HJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom LSTM network"
      ],
      "metadata": {
        "id": "dzl7yfYkGS7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "parameters of LSTM/RNN/GRU layer:\n",
        "\n",
        "input_size – The number of expected features in the input x\n",
        "\n",
        "hidden_size – The number of features in the hidden state h\n",
        "\n",
        "num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM,\n",
        "with the second LSTM taking in outputs of the first LSTM and computing the final results\n",
        "\n",
        "batch_first – If True, then the input and output tensors are provided as (batch, seq, feature)\n",
        "\n",
        "\"\"\"\n",
        "######\n",
        "# hyperparameters\n",
        "#lstm_embeding_dim = ??\n",
        "#lstm_hid_size = ??\n",
        "#lstm_layers=  ??\n",
        "######\n",
        "\n",
        "# Set hyperparameters\n",
        "input_size = n_vocab  # Number of unique characters\n",
        "output_size = n_vocab  # Number of unique characters\n",
        "embedding_dim = 50\n",
        "hidden_size = 100\n",
        "num_layers = 2\n",
        "\n",
        "class CharModel_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, embedding_dim=50):\n",
        "        super(CharModel_LSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.linear(out[:, -1, :])  # Select the output from the last time step\n",
        "        return out"
      ],
      "metadata": {
        "id": "BurEkeMph6wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom RNN Network"
      ],
      "metadata": {
        "id": "fZmhhmyOGXGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "parameters of LSTM/RNN/GRU layer:\n",
        "\n",
        "input_size – The number of expected features in the input x\n",
        "\n",
        "hidden_size – The number of features in the hidden state h\n",
        "\n",
        "num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN,\n",
        "with the second RNN taking in outputs of the first RNN and computing the final results\n",
        "\n",
        "batch_first – If True, then the input and output tensors are provided as (batch, seq, feature)\n",
        "\n",
        "\"\"\"\n",
        "######\n",
        "# hyperparameters\n",
        "#rnn_embeding_dim = ??\n",
        "#rnn_hid_size = ??\n",
        "#rnn_layers=  ??\n",
        "\n",
        "\n",
        "######\n",
        "# Set hyperparameters\n",
        "input_size = n_vocab  # Number of unique characters\n",
        "output_size = n_vocab  # Number of unique characters\n",
        "embedding_dim = 50\n",
        "hidden_size = 100\n",
        "num_layers = 2\n",
        "\n",
        "class CharModel_RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, embedding_dim=50):\n",
        "        super(CharModel_RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.linear(out[:, -1, :])  # Select the output from the last time step\n",
        "        return out"
      ],
      "metadata": {
        "id": "GUCKQgkqGJpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom GRU Net"
      ],
      "metadata": {
        "id": "Ul_0zFEfGb3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "parameters of LSTM/RNN/GRU layer:\n",
        "\n",
        "input_size – The number of expected features in the input x\n",
        "\n",
        "hidden_size – The number of features in the hidden state h\n",
        "\n",
        "num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRU together to form a stacked GRU,\n",
        "with the second GRU taking in outputs of the first GRU and computing the final results\n",
        "\n",
        "batch_first – If True, then the input and output tensors are provided as (batch, seq, feature)\n",
        "\n",
        "\"\"\"\n",
        "######\n",
        "# hyperparameters\n",
        "#gru_embeding_dim = ??\n",
        "#gru_hid_size = ??\n",
        "#gru_layers=  ??\n",
        "\n",
        "\n",
        "######\n",
        "# Set hyperparameters\n",
        "input_size = n_vocab  # Number of unique characters\n",
        "output_size = n_vocab  # Number of unique characters\n",
        "embedding_dim = 50\n",
        "hidden_size = 100\n",
        "num_layers = 2\n",
        "\n",
        "\n",
        "\n",
        "class CharModel_GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, embedding_dim=50):\n",
        "        super(CharModel_GRU, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.gru(x)\n",
        "        out = self.linear(out[:, -1, :])  # Select the output from the last time step\n",
        "        return out"
      ],
      "metadata": {
        "id": "aeBoWC3TGaqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate models\n",
        "#lstm_model = CharModel_LSTM(input_size, hidden_size, num_layers, output_size, embedding_dim)\n",
        "#rnn_model = CharModel_RNN(input_size, hidden_size, num_layers, output_size, embedding_dim)\n",
        "#gru_model = CharModel_GRU(input_size, hidden_size, num_layers, output_size, embedding_dim)"
      ],
      "metadata": {
        "id": "pgo1b7kSHLw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "input_size = n_vocab  # Number of unique characters\n",
        "output_size = n_vocab  # Number of unique characters\n",
        "embedding_dim = 50\n",
        "hidden_size = 100\n",
        "num_layers = 2\n",
        "\n",
        "# Check for GPU availability\n",
        "model = CharModel_LSTM(input_size, hidden_size, num_layers, output_size, embedding_dim)\n",
        "\n",
        "\n",
        "if torch.cuda.is_available:\n",
        "  print('cuda available')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Now you can use the lstm_model for training or inference.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpBfPIqaWgYe",
        "outputId": "d3489a14-0ff9-4d47-d4b8-a55658b0f0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda available\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharModel_LSTM(\n",
              "  (embedding): Embedding(48, 50)\n",
              "  (lstm): LSTM(50, 100, num_layers=2, batch_first=True)\n",
              "  (linear): Linear(in_features=100, out_features=48, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "e7hNtilqictV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write code to train the LSTM network and store the model as a checkpoint. Use validation data to tune your hyperparameters learning rate, batch size, num epochs, etc."
      ],
      "metadata": {
        "id": "zeZ91GodGygG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"model = CharModel_LSTM()\n",
        "if torch.cuda.is_available:\n",
        "  print('cuda available')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\"\"\""
      ],
      "metadata": {
        "id": "ASHx0j8EigEW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2daa3c0a-b554-478f-ce9c-cd421f0e0139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model = CharModel_LSTM()\\nif torch.cuda.is_available:\\n  print(\\'cuda available\\')\\n\\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\\nmodel.to(device)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the parameters for training,\n",
        "# tune the tunable parameters if necessary using val data\n",
        "batch_size = 40\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "train_loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
        "val_loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
        "test_loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "vJVSWyfyXA-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "n_epochs = 40\n",
        "best_model = None\n",
        "### ckpt_path =  # set up check point path\n",
        "## If you have stored a checkpoint, In this way you can resume training model after the last training step by just loading it from the directory given.\n",
        "# if os.path.isfile(ckpt_path):\n",
        "#     model.load_state_dict(torch.load(ckpt_path)[0])\n",
        "#     print('loading from model check point')\n",
        "\n",
        "best_loss = np.inf\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    #Write training code\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        y_pred = model(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred.to(device), y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Validation\n",
        "    # Writevalidation code and tune parameters if necessary\n",
        "    #Include codes to print necessary logs\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            y_pred = model(X_batch.to(device))\n",
        "            loss += loss_fn(y_pred.to(device), y_batch.to(device))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model = model.state_dict()\n",
        "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n",
        "torch.save([best_model, char_to_int], \"single-char.pth\")\n"
      ],
      "metadata": {
        "id": "HMyMZYkjjWgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f23f70-b9a4-4a9d-f9cb-2b6e3dd43d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Cross-entropy: 832755.0625\n",
            "Epoch 1: Cross-entropy: 777695.0000\n",
            "Epoch 2: Cross-entropy: 749372.1875\n",
            "Epoch 3: Cross-entropy: 735536.3125\n",
            "Epoch 4: Cross-entropy: 726456.3750\n",
            "Epoch 5: Cross-entropy: 715060.0625\n",
            "Epoch 6: Cross-entropy: 705209.6875\n",
            "Epoch 7: Cross-entropy: 702151.1250\n",
            "Epoch 8: Cross-entropy: 696727.9375\n",
            "Epoch 9: Cross-entropy: 695195.1250\n",
            "Epoch 10: Cross-entropy: 687252.4375\n",
            "Epoch 11: Cross-entropy: 686725.1875\n",
            "Epoch 12: Cross-entropy: 683332.9375\n",
            "Epoch 13: Cross-entropy: 682335.1875\n",
            "Epoch 14: Cross-entropy: 674999.8125\n",
            "Epoch 15: Cross-entropy: 674161.0000\n",
            "Epoch 16: Cross-entropy: 673590.7500\n",
            "Epoch 17: Cross-entropy: 670990.5625\n",
            "Epoch 18: Cross-entropy: 669421.3750\n",
            "Epoch 19: Cross-entropy: 668560.2500\n",
            "Epoch 20: Cross-entropy: 668059.8750\n",
            "Epoch 21: Cross-entropy: 665401.6250\n",
            "Epoch 22: Cross-entropy: 662319.3750\n",
            "Epoch 23: Cross-entropy: 662683.5000\n",
            "Epoch 24: Cross-entropy: 662400.8750\n",
            "Epoch 25: Cross-entropy: 663001.6250\n",
            "Epoch 26: Cross-entropy: 660435.6250\n",
            "Epoch 27: Cross-entropy: 659373.8750\n",
            "Epoch 28: Cross-entropy: 660170.3750\n",
            "Epoch 29: Cross-entropy: 658508.3750\n",
            "Epoch 30: Cross-entropy: 655488.3750\n",
            "Epoch 31: Cross-entropy: 656560.4375\n",
            "Epoch 32: Cross-entropy: 656439.5625\n",
            "Epoch 33: Cross-entropy: 655922.8125\n",
            "Epoch 34: Cross-entropy: 656593.6250\n",
            "Epoch 35: Cross-entropy: 653441.3125\n",
            "Epoch 36: Cross-entropy: 651356.7500\n",
            "Epoch 37: Cross-entropy: 655155.8750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating character using trained model"
      ],
      "metadata": {
        "id": "Arfwa2N2keC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, char_to_int = torch.load(\"single-char.pth\")  ## loading saved model\n",
        "n_vocab = len(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
        "model.load_state_dict(best_model)"
      ],
      "metadata": {
        "id": "7X3qDS7mkjIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66304442-edb7-46e1-9b8f-9b64f673a0cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_QFuGTOFYLiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp0LLY0neFBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a5ffac-5681-406e-9802-02ea8472710f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: \"tensor([18, 11, 32, 19, 24, 17,  0], dtype=torch.int32)\"\n",
            "Next chars (actual): tensor(11)\n",
            "----\n",
            "prediction:\n",
            "\n",
            "\n",
            "--------\n"
          ]
        }
      ],
      "source": [
        "#Test your model's generative performance using the following testing code\n",
        "# randomly generate a prompt\n",
        "seq_length = 7\n",
        "n =10 ## number of characters to predict\n",
        "rand_index = np.random.choice(len(X_test))\n",
        "prompt = X_test[rand_index]\n",
        "next_char_actual = y_test[rand_index]\n",
        "\n",
        "# Convert the PyTorch tensor to a list of integers\n",
        "prompt_list = prompt.tolist()\n",
        "# Map the list of integers to characters using char_to_int dictionary\n",
        "pattern = [char_to_int.get(c, '<unknown>') for c in prompt_list]\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------------#\n",
        "#prediction\n",
        "\n",
        "model.eval()\n",
        "print('Prompt: \"%s\"' % prompt)\n",
        "print('Next chars (actual):',next_char_actual )\n",
        "print('----')\n",
        "print('prediction:\\n')\n",
        "#Use your model to predict the next seq_length characters and check the performance\n",
        "print('\\n--------')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use your model to predict the next seq_length characters and check the performance\n",
        "seq_length = 7\n",
        "raw_text = open(path, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "raw_text = remove_punc(raw_text)\n",
        "start = np.random.randint(0, len(raw_text)-seq_length)\n",
        "prompt = raw_text[start:start+seq_length]\n",
        "pattern = [char_to_int[c] for c in prompt]\n",
        "with torch.no_grad():\n",
        "    for i in range(n):\n",
        "        # format input array of int into PyTorch tensor\n",
        "        x = torch.tensor(np.reshape(pattern, (1, len(pattern))), dtype=torch.long)  # Use dtype=torch.long\n",
        "        # generate logits as output from the model\n",
        "        prediction = model(x.to(device))\n",
        "        # convert logits into one character\n",
        "        index = int(prediction.argmax())\n",
        "        result = int_to_char[index]\n",
        "        print(result, end=\"\")\n",
        "        # append the new character into the prompt for the next iteration\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:]\n",
        "print()\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qZbl403ozZf",
        "outputId": "3c71d0a3-7a64-440f-a587-71c3e09dea10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d not see \n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use your model to predict the next seq_length characters and check the performance\n",
        "# CODE HERE\n",
        "seq_length = 7\n",
        "raw_text = open(path, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "raw_text = remove_punc(raw_text)\n",
        "start = np.random.randint(0, len(raw_text)-seq_length)\n",
        "prompt = raw_text[start:start+seq_length]\n",
        "pattern = [char_to_int[c] for c in prompt]\n",
        "with torch.no_grad():\n",
        "    for i in range(n):\n",
        "        # format input array of int into PyTorch tensor\n",
        "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "        # generate logits as output from the model\n",
        "        prediction = model(x.to())\n",
        "        # convert logits into one character\n",
        "        index = int(prediction.argmax())\n",
        "        result = int_to_char[index]\n",
        "        print(result, end=\"\")\n",
        "        # append the new character into the prompt for the next iteration\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:]\n",
        "print()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "DInlWoF_Ye3q",
        "outputId": "43a2033e-3496-4c07-a166-9211cd168c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-85b4d085880f>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# generate logits as output from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# convert logits into one character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-2be6f256944c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Select the output from the last time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Using the template above, train the RNN network and store the model as a checkpoint. Use validation data to tune your hyperparameters learning rate, batch size, num epochs, etc. And test the models performance on an arbitrary test data point (or a set of test data points). Also evaluate the performance on full test data using character error rate."
      ],
      "metadata": {
        "id": "cR1H4gp7I1Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5hGYNwZprGb",
        "outputId": "ab966f00-247f-4b8c-a021-5c989d67fbcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.23.0 rapidfuzz-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from Levenshtein import distance  # For calculating Character Error Rate (CER)\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "n_epochs = 20\n",
        "\n",
        "# Instantiate RNN model\n",
        "rnn_model = CharModel_RNN(input_size, hidden_size, num_layers, output_size, embedding_dim)\n",
        "rnn_model.to(device)\n",
        "\n",
        "# Set up optimizer and loss function\n",
        "optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "# Set up data loaders for training, validation, and test data\n",
        "train_loader = DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(data.TensorDataset(X_val, y_val), shuffle=False, batch_size=batch_size)\n",
        "test_loader = DataLoader(data.TensorDataset(X_test, y_test), shuffle=False, batch_size=batch_size)\n",
        "\n",
        "# Training loop\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    rnn_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{n_epochs}'):\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        y_pred = rnn_model(X_batch)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    rnn_model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_val_batch, y_val_batch in val_loader:\n",
        "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
        "            y_val_pred = rnn_model(X_val_batch)\n",
        "            val_loss += loss_fn(y_val_pred, y_val_batch).item()\n",
        "\n",
        "    average_val_loss = val_loss / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{n_epochs}, Training Loss: {average_loss:.4f}, Validation Loss: {average_val_loss:.4f}\")\n",
        "\n",
        "    # Save the best model based on validation loss\n",
        "    if average_val_loss < best_loss:\n",
        "        best_loss = average_val_loss\n",
        "        best_model = rnn_model.state_dict()\n",
        "\n",
        "# Save the best model as a checkpoint\n",
        "torch.save(best_model, \"rnn_model_checkpoint.pth\")\n",
        "\n",
        "# Load the best model from the checkpoint\n",
        "rnn_model.load_state_dict(torch.load(\"rnn_model_checkpoint.pth\"))\n",
        "rnn_model.eval()\n",
        "\n",
        "# Evaluate performance on an arbitrary test data point\n",
        "seq_length = 7\n",
        "start = np.random.randint(0, len(X_test)-seq_length)\n",
        "prompt = X_test[start:start+seq_length].to(device)\n",
        "next_char_actual = y_test[start+seq_length-1]\n",
        "\n",
        "pattern = [char_to_int[c] for c in prompt.tolist()]\n",
        "\n",
        "# Prediction\n",
        "predicted_chars = []\n",
        "with torch.no_grad():\n",
        "    for i in range(n):\n",
        "        x = torch.tensor(np.reshape(pattern, (1, len(pattern))), dtype=torch.long).to(device)\n",
        "        prediction = rnn_model(x)\n",
        "        index = int(prediction.argmax())\n",
        "        result = int_to_char[index]\n",
        "        predicted_chars.append(result)\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:]\n",
        "\n",
        "predicted_sequence = \"\".join(predicted_chars)\n",
        "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
        "print(f\"Next chars (actual): {int_to_char[next_char_actual]}\")\n",
        "print(f\"Prediction: {predicted_sequence}\")\n",
        "\n",
        "# Evaluate performance on the full test data using Character Error Rate (CER)\n",
        "def calculate_cer(predictions, targets):\n",
        "    return sum(distance(prediction, target) for prediction, target in zip(predictions, targets)) / sum(len(target) for target in targets)\n",
        "\n",
        "test_predictions = []\n",
        "test_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_test_batch, y_test_batch in tqdm(test_loader, desc=\"Evaluating on test data\"):\n",
        "        X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
        "        y_test_pred = rnn_model(X_test_batch)\n",
        "        predicted_indices = y_test_pred.argmax(dim=2)\n",
        "        test_predictions.extend([\"\".join(int_to_char[i.item()] for i in indices) for indices in predicted_indices])\n",
        "        test_targets.extend([\"\".join(int_to_char[i.item()] for i in seq) for seq in y_test_batch])\n",
        "\n",
        "cer = calculate_cer(test_predictions, test_targets)\n",
        "print(f\"Character Error Rate (CER) on the full test data: {cer:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_KEwKasBpdRE",
        "outputId": "b278a89c-6ab9-4b20-e97b-8f73054e9b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 9208/9208 [00:31<00:00, 290.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Training Loss: 1.6386, Validation Loss: 1.4763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 9208/9208 [00:27<00:00, 332.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20, Training Loss: 1.4320, Validation Loss: 1.4141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 9208/9208 [00:27<00:00, 339.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20, Training Loss: 1.3833, Validation Loss: 1.3828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 9208/9208 [00:26<00:00, 343.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20, Training Loss: 1.3580, Validation Loss: 1.3663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 9208/9208 [00:27<00:00, 337.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20, Training Loss: 1.3424, Validation Loss: 1.3578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 9208/9208 [00:27<00:00, 339.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20, Training Loss: 1.3311, Validation Loss: 1.3488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 9208/9208 [00:25<00:00, 365.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20, Training Loss: 1.3230, Validation Loss: 1.3461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 9208/9208 [00:25<00:00, 358.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20, Training Loss: 1.3170, Validation Loss: 1.3422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 9208/9208 [00:26<00:00, 350.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20, Training Loss: 1.3117, Validation Loss: 1.3391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 9208/9208 [00:26<00:00, 348.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20, Training Loss: 1.3083, Validation Loss: 1.3339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 9208/9208 [00:27<00:00, 339.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20, Training Loss: 1.3053, Validation Loss: 1.3321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 9208/9208 [00:26<00:00, 343.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20, Training Loss: 1.3033, Validation Loss: 1.3280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 9208/9208 [00:26<00:00, 352.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20, Training Loss: 1.3014, Validation Loss: 1.3294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 9208/9208 [00:25<00:00, 359.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20, Training Loss: 1.2994, Validation Loss: 1.3330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 9208/9208 [00:24<00:00, 368.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20, Training Loss: 1.2993, Validation Loss: 1.3269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 9208/9208 [00:27<00:00, 332.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20, Training Loss: 1.2978, Validation Loss: 1.3313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 9208/9208 [00:26<00:00, 346.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20, Training Loss: 1.2967, Validation Loss: 1.3300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 9208/9208 [00:26<00:00, 342.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20, Training Loss: 1.2968, Validation Loss: 1.3386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 9208/9208 [00:26<00:00, 342.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20, Training Loss: 1.2961, Validation Loss: 1.3277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 9208/9208 [00:26<00:00, 342.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20, Training Loss: 1.2956, Validation Loss: 1.3264\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-cec101b0b149>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mnext_char_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-cec101b0b149>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mnext_char_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Using the template above, train the GRU network and store the model as a checkpoint. Use validation data to tune your hyperparameters learning rate, batch size, num epochs, etc. And test the models performance on an arbitrary test data point (or a set of test data points). Also evaluate the performance on full test data using character error rate."
      ],
      "metadata": {
        "id": "aNBINVP1JCIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from Levenshtein import distance  # For calculating Character Error Rate (CER)\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "n_epochs = 20\n",
        "\n",
        "# Instantiate GRU model\n",
        "gru_model = CharModel_GRU(input_size, hidden_size, num_layers, output_size, embedding_dim)\n",
        "gru_model.to(device)\n",
        "\n",
        "# Set up optimizer and loss function\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "# Set up data loaders for training, validation, and test data\n",
        "train_loader = DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(data.TensorDataset(X_val, y_val), shuffle=False, batch_size=batch_size)\n",
        "test_loader = DataLoader(data.TensorDataset(X_test, y_test), shuffle=False, batch_size=batch_size)\n",
        "\n",
        "# Training loop\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    gru_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{n_epochs}'):\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        y_pred = gru_model(X_batch)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    gru_model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_val_batch, y_val_batch in val_loader:\n",
        "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
        "            y_val_pred = gru_model(X_val_batch)\n",
        "            val_loss += loss_fn(y_val_pred, y_val_batch).item()\n",
        "\n",
        "    average_val_loss = val_loss / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{n_epochs}, Training Loss: {average_loss:.4f}, Validation Loss: {average_val_loss:.4f}\")\n",
        "\n",
        "    # Save the best model based on validation loss\n",
        "    if average_val_loss < best_loss:\n",
        "        best_loss = average_val_loss\n",
        "        best_model = gru_model.state_dict()\n",
        "\n",
        "# Save the best model as a checkpoint\n",
        "torch.save(best_model, \"gru_model_checkpoint.pth\")\n",
        "\n",
        "# Load the best model from the checkpoint\n",
        "gru_model.load_state_dict(torch.load(\"gru_model_checkpoint.pth\"))\n",
        "gru_model.eval()\n",
        "\n",
        "# Evaluate performance on an arbitrary test data point\n",
        "seq_length = 7\n",
        "start = np.random.randint(0, len(X_test)-seq_length)\n",
        "prompt = X_test[start:start+seq_length].to(device)\n",
        "next_char_actual = y_test[start+seq_length-1]\n",
        "\n",
        "pattern = [char_to_int[c] for c in prompt.tolist()]\n",
        "\n",
        "# Prediction\n",
        "predicted_chars = []\n",
        "with torch.no_grad():\n",
        "    for i in range(n):\n",
        "        x = torch.tensor(np.reshape(pattern, (1, len(pattern))), dtype=torch.long).to(device)\n",
        "        prediction = gru_model(x)\n",
        "        index = int(prediction.argmax())\n",
        "        result = int_to_char[index]\n",
        "        predicted_chars.append(result)\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:]\n",
        "\n",
        "predicted_sequence = \"\".join(predicted_chars)\n",
        "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
        "print(f\"Next chars (actual): {int_to_char[next_char_actual]}\")\n",
        "print(f\"Prediction: {predicted_sequence}\")\n",
        "\n",
        "# Evaluate performance on the full test data using Character Error Rate (CER)\n",
        "test_predictions = []\n",
        "test_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_test_batch, y_test_batch in tqdm(test_loader, desc=\"Evaluating on test data\"):\n",
        "        X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
        "        y_test_pred = gru_model(X_test_batch)\n",
        "        predicted_indices = y_test_pred.argmax(dim=2)\n",
        "        test_predictions.extend([\"\".join(int_to_char[i.item()] for i in indices) for indices in predicted_indices])\n",
        "        test_targets.extend([\"\".join(int_to_char[i.item()] for i in seq) for seq in y_test_batch])\n",
        "\n",
        "cer = calculate_cer(test_predictions, test_targets)\n",
        "print(f\"Character Error Rate (CER) on the full test data: {cer:.4f}\")\n"
      ],
      "metadata": {
        "id": "twqdwH0mJAOg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b53d7928-ef27-4c20-d981-5978bca17626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 9208/9208 [00:34<00:00, 267.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Training Loss: 1.5718, Validation Loss: 1.4088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 9208/9208 [00:28<00:00, 326.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20, Training Loss: 1.3569, Validation Loss: 1.3436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 9208/9208 [00:27<00:00, 330.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20, Training Loss: 1.3073, Validation Loss: 1.3171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 9208/9208 [00:35<00:00, 263.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20, Training Loss: 1.2806, Validation Loss: 1.3002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 9208/9208 [00:28<00:00, 318.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20, Training Loss: 1.2643, Validation Loss: 1.2911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 9208/9208 [00:29<00:00, 314.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20, Training Loss: 1.2533, Validation Loss: 1.2830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 9208/9208 [00:28<00:00, 322.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20, Training Loss: 1.2447, Validation Loss: 1.2830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 9208/9208 [00:28<00:00, 324.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20, Training Loss: 1.2383, Validation Loss: 1.2789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 9208/9208 [00:29<00:00, 309.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20, Training Loss: 1.2334, Validation Loss: 1.2726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 9208/9208 [00:29<00:00, 315.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20, Training Loss: 1.2293, Validation Loss: 1.2732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 9208/9208 [00:29<00:00, 308.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20, Training Loss: 1.2258, Validation Loss: 1.2751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 9208/9208 [00:28<00:00, 324.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20, Training Loss: 1.2243, Validation Loss: 1.2720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 9208/9208 [00:28<00:00, 318.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20, Training Loss: 1.2218, Validation Loss: 1.2703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 9208/9208 [00:29<00:00, 311.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20, Training Loss: 1.2190, Validation Loss: 1.2717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 9208/9208 [00:28<00:00, 319.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20, Training Loss: 1.2201, Validation Loss: 1.2734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 9208/9208 [00:28<00:00, 328.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20, Training Loss: 1.2184, Validation Loss: 1.2752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 9208/9208 [00:28<00:00, 326.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20, Training Loss: 1.2175, Validation Loss: 1.2712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 9208/9208 [00:30<00:00, 306.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20, Training Loss: 1.2178, Validation Loss: 1.2726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 9208/9208 [00:28<00:00, 321.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20, Training Loss: 1.2178, Validation Loss: 1.2733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 9208/9208 [00:29<00:00, 313.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20, Training Loss: 1.2173, Validation Loss: 1.2742\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-e757fd9f2210>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mnext_char_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-e757fd9f2210>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mnext_char_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    }
  ]
}